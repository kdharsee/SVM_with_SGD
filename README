Name: Komail Dharsee
Email: kdharsee@cs.rochester.edu
Course CSC446
Homework:
We were tasked with implenting the perceptron algorithm in Python and experimenting the learning rate's effect on performance.

******* Instructions for Running perceptron.py *******
Usage: python perceptron.py <training file> <dev file> <test file>

******* Implementation *******
Each value for the learning rate was trained over 50 iterations on the dev set. Values from 1 to 0, with steps of 0.01 were all tested to search for the best learning rate, while updating the weight vector throughout the process. The final weight vector was then used on the Testing set.

The learning rate was implemented such that larger learning rates were implemented in initial updates to the weight vector. With further iterations, the learning rate would decrease, hopefully leading to a more consistent accuracy -- or less overfitting.

******* Results *******
Training with decrementing learning rates from 1 to 0 resulted in an accuracy of approximately 80%. The following list of numbers describes corresponding learning rates and their accuracies achieved over 50 iterations on the dev set. (<learning rate>, <dev set accuracy>)

(0.98999999999999999, 0.779875)
(0.97999999999999998, 0.771875)
(0.96999999999999997, 0.826875)
(0.95999999999999996, 0.817875)
(0.95000000000000007, 0.741375)
(0.94000000000000006, 0.74675)
(0.93000000000000005, 0.770375)
(0.92000000000000004, 0.806875)
(0.91000000000000003, 0.806625)
(0.90000000000000002, 0.7975)
(0.89000000000000001, 0.7805)
(0.88, 0.76925)
(0.87, 0.814125)
(0.85999999999999999, 0.820125)
(0.84999999999999998, 0.77375)
(0.83999999999999997, 0.806875)
(0.83000000000000007, 0.769)
(0.82000000000000006, 0.826)
(0.81000000000000005, 0.783875)
(0.80000000000000004, 0.797125)
(0.79000000000000004, 0.77225)
(0.78000000000000003, 0.795875)
(0.77000000000000002, 0.812125)
(0.76000000000000001, 0.78825)
(0.75, 0.80075)
(0.73999999999999999, 0.741)
(0.72999999999999998, 0.76425)
(0.71999999999999997, 0.7985)
(0.70999999999999996, 0.790375)
(0.70000000000000007, 0.7725)
(0.69000000000000006, 0.74425)
(0.68000000000000005, 0.818)
(0.67000000000000004, 0.803625)
(0.66000000000000003, 0.809875)
(0.65000000000000002, 0.794125)
(0.64000000000000001, 0.814375)
(0.63, 0.76775)
(0.62, 0.804125)
(0.60999999999999999, 0.7915)
(0.59999999999999998, 0.801625)
(0.58999999999999997, 0.781)
(0.57999999999999996, 0.772625)
(0.57000000000000006, 0.78575)
(0.56000000000000005, 0.706625)
(0.55000000000000004, 0.793625)
(0.54000000000000004, 0.79875)
(0.53000000000000003, 0.793125)
(0.52000000000000002, 0.825375)
(0.51000000000000001, 0.812375)
(0.5, 0.818)
(0.48999999999999999, 0.713625)
(0.47999999999999998, 0.797375)
(0.47000000000000003, 0.822125)
(0.46000000000000002, 0.744875)
(0.45000000000000001, 0.764125)
(0.44, 0.778875)
(0.42999999999999999, 0.79425)
(0.41999999999999998, 0.770125)
(0.41000000000000003, 0.814875)
(0.40000000000000002, 0.777375)
(0.39000000000000001, 0.805)
(0.38, 0.803625)
(0.37, 0.815375)
(0.35999999999999999, 0.793)
(0.35000000000000003, 0.775625)
(0.34000000000000002, 0.728375)
(0.33000000000000002, 0.789625)
(0.32000000000000001, 0.724375)
(0.31, 0.803)
(0.29999999999999999, 0.76325)
(0.28999999999999998, 0.739625)
(0.28000000000000003, 0.79)
(0.27000000000000002, 0.76275)
(0.26000000000000001, 0.79875)
(0.25, 0.80475)
(0.23999999999999999, 0.724125)
(0.23000000000000001, 0.731875)
(0.22, 0.800875)
(0.20999999999999999, 0.776875)
(0.20000000000000001, 0.7565)
(0.19, 0.806)
(0.17999999999999999, 0.726375)
(0.17000000000000001, 0.801625)
(0.16, 0.7955)
(0.14999999999999999, 0.811375)
(0.14000000000000001, 0.774375)
(0.13, 0.788)
(0.12, 0.768625)
(0.11, 0.772375)
(0.10000000000000001, 0.756)
(0.089999999999999997, 0.7965)
(0.080000000000000002, 0.82275)
(0.070000000000000007, 0.796625)
(0.059999999999999998, 0.802375)
(0.050000000000000003, 0.78825)
(0.040000000000000001, 0.772125)
(0.029999999999999999, 0.80275)
(0.02, 0.803125)
(0.01, 0.809875)
(0.0, 0.809875)
[0.96999999999999997, 0.826875]
Testing Accuracy: 0.80309656069

******* Interpretation *******
A noticable pattern as we reached lower numbers of learning rates showed that the fluctuation of dev set accuracy decreased considerably as compared to the fluctuation with higher learning rates. Such an observation follows in line with the theory that updates to the weight vector with higher learning rates incur larger shifts in the vector.

