Name: Komail Dharsee
Email: kdharsee@cs.rochester.edu
Course CSC446
Homework:
We were tasked with implenting a Support Vector Machine using Stochastic Gradient Descent (SVM with SGD) in Python. We were then to experiment with learning and slack rates to better performance.

******* Files *******
svm.py README

******* Instructions for Running svm.py *******
Usage: python svm.py <training file> <dev file> <test file>

******* Implementation ******* 

Each value for the learning rate was trained over 10
iterations on the dev set with values from 1 to 0, and
in steps of 0.01. The slack variable 'C' was also
similarly tweaked between values of 1 and 0 with steps
of 0.01.

******* Results *******

From tweaking the slack and learning rate, it was found
that performance was increased on average with higher
slack rates, closer to 0.98. Performance for the
classifier further increased with smaller learning
rates combined with relatively lower numbers of
iterations as compared to what was used for the
perceptron project.

The highest accuracy, of 84.8%, on the dev set was
achieved with the Slack variable set to 0.98 and
learning rate to 0.01 with 10 iterations. Increasing
the number of iteration passed 50 seemed to introduce
detrimental effects on performance, drastically
reducing the accuracy of the classifier. .

The following series shows the combinations of tuning
variables on the dev set (C, learning rate, accuracy)
which achieved accuracies above 84%:

(0.9900000000000001, 0.040000000000000001, 0.84025)
(0.90000000000000002, 0.029999999999999999, 0.842875)
(0.91000000000000003, 0.029999999999999999, 0.84275)
(0.92000000000000004, 0.029999999999999999, 0.842375)
(0.93000000000000005, 0.029999999999999999, 0.842)
(0.94000000000000006, 0.029999999999999999, 0.841625)
(0.95000000000000007, 0.029999999999999999, 0.841875)
(0.96000000000000008, 0.029999999999999999, 0.84225)
(0.97000000000000008, 0.029999999999999999, 0.84225)
(0.98000000000000009, 0.029999999999999999, 0.841625)
(0.9900000000000001, 0.029999999999999999, 0.84375)
(0.90000000000000002, 0.02, 0.845875)
(0.91000000000000003, 0.02, 0.84625)
(0.92000000000000004, 0.02, 0.84475)
(0.93000000000000005, 0.02, 0.844625)
(0.94000000000000006, 0.02, 0.845375)
(0.95000000000000007, 0.02, 0.84575)
(0.96000000000000008, 0.02, 0.845375)
(0.97000000000000008, 0.02, 0.84625)
(0.98000000000000009, 0.02, 0.845375)
(0.9900000000000001, 0.02, 0.84425)
(0.90000000000000002, 0.01, 0.8475)
(0.91000000000000003, 0.01, 0.846625)
(0.92000000000000004, 0.01, 0.84675)
(0.93000000000000005, 0.01, 0.847125)
(0.94000000000000006, 0.01, 0.847375)
(0.95000000000000007, 0.01, 0.846375)
(0.96000000000000008, 0.01, 0.847625)
(0.97000000000000008, 0.01, 0.847375)
(0.98000000000000009, 0.01, 0.84775)
(0.9900000000000001, 0.01, 0.84725)

******* Interpretation *******

It was expected that introducing more slack to the classifer would allow for some better performance due to the unlikely circumstances of data to be perfectly linearly separable. The slack would allow some data points to lay within the margin defined by the support vectors, rather than the alternative case of no slack where there would not exist any data points laying within the margin. 

It was generally expected over this experiment once more that having a lower learning rate would be useful in lowering the effect of over fitting. It would be interesting to further expand on the learning rate, allowing for a variable learning rate based off the number of iterations already completed. 

